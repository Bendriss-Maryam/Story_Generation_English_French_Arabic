# Children_Story_Generation_English_French_Arabic
We focus on this project on fine-tuning an OpenAI GPT-2 pre-trained model for generating children stories in three languages (English,French,Arabic). 

Deep learning and pre-training models have demonstrated excellent results in several language tasks recently. Particularly, finetuning the pretrained models such as ELMo, OpenAI GPT, GPT-2 and BERT has become the best practice for state-of-the-art results. GPT-2 is the successor of GPT. Although both GPT-2 and BERT are capable of text generation, Wang and Cho found that GPT-2 generations are much better in terms of quality. In fact, GPT-2 is claimed to be so powerful that the risk of its malicious use is high. For this reason, OpenAI decided to keep its largest model (1.5B parameters) closed so that there is more time to discuss its ramifications.
In this work, we generated children stories by fine-tuning the released 345M medium version. Overall, we are impressed by how coherent the generated story could be, although not all text are generated equally in terms of quality. We are also surprised by how few training steps were necessary for GPT-2 to generate the first text that looks like a real story. It is a matter of time that the largest and more powerful model will be released to the public. Therefore, it is better to experiment on GPT-2 and contemplate on its impact on text generation from the beginning of GPT-2 development.

English Stories : 
Fine Tuning GPT-2 using simple-Transformers Library : 


